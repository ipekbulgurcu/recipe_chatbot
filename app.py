import os
import streamlit as st

# LangChain V0.2.x yerine V0.3.x+ uyumluluÄŸu iÃ§in importlarÄ± gÃ¼ncelliyoruz
from langchain import globals  # Gerekli olabilir
from langchain.chains import create_retrieval_chain
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.documents import Document  # Document sÄ±nÄ±fÄ± iÃ§in

# Google Modelleri
from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings

# Topluluk BileÅŸenleri (Bu, hatalÄ± kÄ±smÄ± aÅŸar)
from langchain_community.document_loaders import TextLoader 
from langchain_community.vectorstores import Chroma

# --- 1. YardÄ±mcÄ± Fonksiyon: RAG Zincirini BaÅŸlatma (HafÄ±zada tutmak iÃ§in) ---
@st.cache_resource
def setup_rag_chain():
    # API Anahtar KontrolÃ¼ (Streamlit Secrets'Ä± kullanmak en gÃ¼venli yoldur)
    if "GEMINI_API_KEY" not in os.environ:
        st.error("âŒ HATA: GEMINI_API_KEY ortam deÄŸiÅŸkeni veya Streamlit Secret ayarlanmadÄ±. LÃ¼tfen ayarlayÄ±n.")
        return None

    # Modelleri baÅŸlat
    llm = ChatGoogleGenerativeAI(model="gemini-2.5-flash", temperature=0.2)
    embeddings = GoogleGenerativeAIEmbeddings(model="text-embedding-004")

    # Veri YÃ¼kleme ve VektÃ¶r VeritabanÄ± (Ã‡Ã¶zÃ¼m Mimariniz)
    file_path = "3000 Yemek Tarifi.txt"
    try:
        loader = TextLoader(file_path, encoding='utf-8')
        documents = loader.load()

        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=1500,
            chunk_overlap=100,
            separators=["TARÄ°F:", "YAPILIÅ TARÄ°FÄ°", "MALZEMELER", "\n\n", "\n", " ", ""]
        )
        chunks = text_splitter.split_documents(documents)

        vector_store = Chroma.from_documents(
            documents=chunks,
            embedding=embeddings
        )
        retriever = vector_store.as_retriever(search_kwargs={"k": 3})

        # Prompt Åablonu
        system_prompt = (
            "Sen harika bir 3000 Yemek Tarifleri AsistanÄ±sÄ±n. YalnÄ±zca aÅŸaÄŸÄ±daki 'context' "
            "iÃ§indeki bilgilere dayanarak kullanÄ±cÄ±nÄ±n tarif ve malzeme sorularÄ±nÄ± TÃ¼rkÃ§e yanÄ±tla. "
            "CevabÄ±nda tarifi net ve adÄ±m adÄ±m aÃ§Ä±kla. "
            "EÄŸer tarif mevcut deÄŸilse, 'Elimde bu tarife ait bilgi bulunmuyor.' ÅŸeklinde kibarca cevap ver."
            "\n\nContext: {context}"
        )
        qa_prompt = ChatPromptTemplate.from_messages([
            ("system", system_prompt),
            ("human", "{input}"),
        ])

        document_chain = create_stuff_documents_chain(llm, qa_prompt)
        rag_chain = create_retrieval_chain(retriever, document_chain)

        return rag_chain

    except FileNotFoundError:
        st.error(f"âŒ HATA: '{file_path}' dosyasÄ± bulunamadÄ±. LÃ¼tfen Streamlit uygulamasÄ±nÄ±n yanÄ±na koyun.")
        return None
    except Exception as e:
        st.error(f"âŒ HATA: RAG zinciri baÅŸlatÄ±lamadÄ±: {e}")
        return None

# --- 2. Streamlit UygulamasÄ± ---
st.set_page_config(page_title="3000 Yemek Tarifi Chatbotu (GenAI Projesi)", layout="wide")
st.title("ğŸ‘¨â€ğŸ³ 3000 Yemek Tarifi Chatbotu")
st.caption("RAG mimarisi ile 3000 tarife anÄ±nda ulaÅŸÄ±n. (Gemini API + LangChain)")

rag_chain = setup_rag_chain()

if rag_chain:
    # Sohbet geÃ§miÅŸini baÅŸlat
    if "messages" not in st.session_state:
        st.session_state.messages = []

    # GeÃ§miÅŸ mesajlarÄ± gÃ¶rÃ¼ntÃ¼le
    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])

    # KullanÄ±cÄ±dan girdi al ve RAG'Ä± Ã§alÄ±ÅŸtÄ±r
    if prompt := st.chat_input("Hangi yemeÄŸin tarifini istersiniz?"):
        st.session_state.messages.append({"role": "user", "content": prompt})
        with st.chat_message("user"):
            st.markdown(prompt)

        with st.chat_message("assistant"):
            with st.spinner("Tarif aranÄ±yor..."):
                try:
                    response = rag_chain.invoke({"input": prompt})
                    full_response = response['answer']
                    st.markdown(full_response)
                except Exception as e:
                    full_response = "ÃœzgÃ¼nÃ¼m, bir hata oluÅŸtu. LÃ¼tfen Gemini API anahtarÄ±nÄ±zÄ± kontrol edin."
                    st.markdown(full_response)

        st.session_state.messages.append({"role": "assistant", "content": full_response})

        
