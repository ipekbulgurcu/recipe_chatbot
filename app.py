import os
import streamlit as st
import textwrap

# Gerekli Temel Importlar
from langchain_core.prompts import ChatPromptTemplate
from langchain_community.document_loaders import TextLoader 
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import Chroma
from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings

# --- 1. YardÄ±mcÄ± Fonksiyon: RAG Ã‡ekirdeÄŸi (Zincirsiz) ---
@st.cache_resource
def setup_rag_core():
    # API Anahtar KontrolÃ¼
    if "GEMINI_API_KEY" not in os.environ:
        st.error("âŒ HATA: GEMINI_API_KEY ayarlanmadÄ±. LÃ¼tfen Secrets kÄ±smÄ±nÄ± kontrol edin.")
        return None, None

    # Modelleri baÅŸlat
    llm = ChatGoogleGenerativeAI(model="gemini-2.5-flash", temperature=0.2)
    embeddings = GoogleGenerativeAIEmbeddings(model="text-embedding-004")
    
    # Veri YÃ¼kleme ve VektÃ¶r VeritabanÄ±
    file_path = "3000 Yemek Tarifi.txt"
    try:
        loader = TextLoader(file_path, encoding='utf-8')
        documents = loader.load()
        
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=1500,
            chunk_overlap=100,
            separators=["TARÄ°F:", "YAPILIÅ TARÄ°FÄ°", "MALZEMELER", "\n\n", "\n", " ", ""]
        )
        chunks = text_splitter.split_documents(documents)

        # ChromaDB'yi oluÅŸtur
        vector_store = Chroma.from_documents(
            documents=chunks,
            embedding=embeddings
        )
        retriever = vector_store.as_retriever(search_kwargs={"k": 3})
        
        return llm, retriever

    except FileNotFoundError:
        st.error(f"âŒ HATA: '{file_path}' dosyasÄ± bulunamadÄ±.")
        return None, None
    except Exception as e:
        st.error(f"âŒ HATA: Veri altyapÄ±sÄ± kurulurken sorun oluÅŸtu: {e}")
        return None, None

# --- 2. Cevap Ãœretim Fonksiyonu (Generation) ---
def generate_response(llm, retriever, prompt):
    # 1. Retrieval (AlÄ±m): Ä°lgili dokÃ¼manlarÄ± Ã§ek
    retrieved_docs = retriever.invoke(prompt)
    
    # DokÃ¼man iÃ§eriÄŸini birleÅŸtir
    context = "\n\n".join(doc.page_content for doc in retrieved_docs)

    # 2. Generation (Ãœretim): Prompt'u oluÅŸtur
    system_prompt_template = textwrap.dedent("""
        Sen harika bir 3000 Yemek Tarifleri AsistanÄ±sÄ±n. YalnÄ±zca aÅŸaÄŸÄ±daki 'context' 
        iÃ§indeki bilgilere dayanarak kullanÄ±cÄ±nÄ±n tarif ve malzeme sorularÄ±nÄ± TÃ¼rkÃ§e yanÄ±tla. 
        CevabÄ±nda tarifi net ve adÄ±m adÄ±m aÃ§Ä±kla. 
        EÄŸer tarif mevcut deÄŸilse, 'Elimde bu tarife ait bilgi bulunmuyor.' ÅŸeklinde kibarca cevap ver.
        
        Context: 
        {context}
    """)
    
    # Gemini modeline gÃ¶nderilecek nihai prompt
    final_prompt = system_prompt_template.format(context=context) + f"\n\nKullanÄ±cÄ± Sorgusu: {prompt}"

    # 3. Modelden cevabÄ± al
    try:
        response = llm.invoke(final_prompt)
        return response.content
    except Exception as e:
        return f"ÃœzgÃ¼nÃ¼m, Gemini API Ã§aÄŸrÄ±sÄ± sÄ±rasÄ±nda bir hata oluÅŸtu: {e}"

# --- 3. Streamlit UygulamasÄ± ---
st.set_page_config(page_title="3000 Yemek Tarifi Chatbotu (Final Projesi)", layout="wide")
st.title("ğŸ‘¨â€ğŸ³ 3000 Yemek Tarifi Chatbotu")
st.caption("RAG mimarisi ile 3000 tarife anÄ±nda ulaÅŸÄ±n. (Gemini API + LangChain)")

llm, retriever = setup_rag_core()

if llm and retriever:
    if "messages" not in st.session_state:
        st.session_state.messages = []

    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])

    if prompt := st.chat_input("Hangi yemeÄŸin tarifini istersiniz?"):
        st.session_state.messages.append({"role": "user", "content": prompt})
        with st.chat_message("user"):
            st.markdown(prompt)

        with st.chat_message("assistant"):
            with st.spinner("Tarif aranÄ±yor..."):
                full_response = generate_response(llm, retriever, prompt)
                st.markdown(full_response)
        
        st.session_state.messages.append({"role": "assistant", "content": full_response})
